{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LoR_bigram_colab.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VVcxDKXvC8rS","colab_type":"text"},"source":["\n","## The implementation of Ngram Language Model with Logistic Regression\n"]},{"cell_type":"code","metadata":{"id":"PUIOvNf9Diu2","colab_type":"code","outputId":"3c37de9d-92e4-4dfd-b918-ecf83ea746b3","executionInfo":{"status":"ok","timestamp":1569376828350,"user_tz":240,"elapsed":25366,"user":{"displayName":"Yanjun Li","photoUrl":"","userId":"03803147230974575947"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lzhWHt9TDqtV","colab_type":"code","outputId":"7a22e7b9-a409-467c-9e48-5ae6d428c6b9","executionInfo":{"status":"ok","timestamp":1569376964417,"user_tz":240,"elapsed":1875,"user":{"displayName":"Yanjun Li","photoUrl":"","userId":"03803147230974575947"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls /content/gdrive/Shared\\ drives/CISC6210NLPFall19/ExampleCode/bigram_LR/*.py"],"execution_count":6,"outputs":[{"output_type":"stream","text":["'/content/gdrive/Shared drives/CISC6210NLPFall19/ExampleCode/bigram_LR/brown.py'\n","'/content/gdrive/Shared drives/CISC6210NLPFall19/ExampleCode/bigram_LR/LoR_bigram.py'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YmCdA9SiEAI_","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/Shared drives/CISC6210NLPFall19/ExampleCode/bigram_LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3LtY572C8rT","colab_type":"code","outputId":"55a47700-1fe2-4e7c-ad2e-dfb2d16ca5f0","executionInfo":{"status":"ok","timestamp":1569377049819,"user_tz":240,"elapsed":27858,"user":{"displayName":"Yanjun Li","photoUrl":"","userId":"03803147230974575947"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from datetime import datetime\n","\n","import brown"],"execution_count":9,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> brown\n","    Downloading package brown to /root/nltk_data...\n","      Unzipping corpora/brown.zip.\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pCJG3iQfC8rV","colab_type":"code","colab":{}},"source":["def softmax(a):\n","    a = a-a.max() # to avoid numerical overflow\n","    exp_a = np.exp(a)\n","    return exp_a/exp_a.sum(axis=1, keepdims=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_B-lJOtulgUa","colab_type":"code","colab":{}},"source":["#plot a smoothed losses line to reduce variability\n","def smoothed_loss(x, decay=0.99):\n","    y = np.zeros(len(x))\n","    last=0\n","    for t in range(len(x)):\n","      z = decay*last+ (1-decay)*x[t]\n","      y[t]= z/(1-decay**(t+1))\n","      last = z\n","    return y\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdRVNv17C8rX","colab_type":"code","colab":{}},"source":[" def main():\n","    \n","    #training set: 2000 words, 2000 sentences.\n","    sentences, word2idx = brown.get_sentences_with_word2idx_limit_vocab(2000,2000)\n","    #for testing\n","    #sentences, word2idx = brown.get_sentences_with_word2idx_limit_vocab(10,10)\n","    V = len(word2idx)\n","    print(f\"word total: {V}\")\n","    start_idx= word2idx['START']\n","    end_idx = word2idx['END']\n","    print(f'Start index={start_idx} and End index = {end_idx}')\n","    \n","   \n","    #train a logistic model\n","    \n","    for lr in [0.1, 0.05, 0.02, 0.01, 0.001]:\n","      \n","      W = np.random.randn(V,V)/np.sqrt(V) \n","      #initial random values to W of shape V x V\n","      #print(f'W\\n{W}')\n","    \n","      losses=[]\n","      epochs = 10\n","      #lr = 1e-2\n","      print(f\"Learing Rate is {lr}:\")\n","      t0=datetime.now()\n","      for epoch in range(epochs):\n","        print(f\"In iteration NO.{epoch}\")\n","        #suffle sentences each epoch\n","        random.shuffle(sentences) \n","        \n","        j=0 #sentence counter\n","        for sentence in sentences:\n","            #convert sentence into one-hot coded inputs and targets\n","            sentence=[start_idx]+sentence+[end_idx] #pad with start and end tag\n","            #print(sentence)\n","            n = len(sentence)\n","            #print(f\"Length of sentence {n}\")\n","            \n","            # for each sentence of length n, there are n-1 bigrams\n","            inputs = np.zeros((n-1,V))\n","            # all first words of bigrams in the sentence\n","            targets = np.zeros((n-1,V))\n","            # all second words of bigrams in the sentence\n","            inputs[np.arange(n-1), sentence[:n-1]]=1\n","            #the sentence itself, ignoring the end index, shape n-1 x V\n","            targets[np.arange(n-1), sentence[1:]]=1\n","            #the next word of the target, shape n-1 x V\n","            # one-hot encoding of word vectors         \n","            #print (f'inputs:\\n{inputs.shape}')\n","            #print(f'targets:\\n{targets.shape}')\n","            \n","            #get output prediction\n","            #since we are using one-hot encoding, bias term is ignored.\n","            #p(y|x)\n","            predictions = softmax(inputs.dot(W)) #shape n-1 x V\n","            #print(f\"Shape of predictions after softmax {predictions.shape}\")#one for each word in the sentence\n","            #print(f\"predictions:\\n{predictions}\")\n","            #do a gradient descent step\n","            #we perform a Mini-batch Gradient Descent algorithm\n","            d = inputs.T.dot(predictions-targets)\n","            W = W - lr*d \n","            \n","            #keep track of the loss - cross entropy cost function, average loss for each sample            \n","            loss = -np.sum(targets*np.log(predictions))/(n-1)#array multiplication\n","            losses.append(loss)\n","            \n","            \n","                                   \n","            if j%500==0:\n","                print(f\"epoch: {epoch}, sentence: {j}/{len(sentences)}, loss: {loss}\")\n","            \n","            j+=1\n","            \n","        \n","        print(f\"Elapsed time training: {datetime.now()-t0}\")\n","      plt.plot(losses)       \n","      plt.plot(smoothed_loss(losses))\n","      plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BB1W5MUAC8rY","colab_type":"code","outputId":"65e0f1d0-7751-49c1-a9bb-a292cddc8c17","executionInfo":{"status":"ok","timestamp":1569255248717,"user_tz":240,"elapsed":1451321,"user":{"displayName":"Yanjun Li","photoUrl":"","userId":"03803147230974575947"}},"colab":{"base_uri":"https://localhost:8080/","height":668}},"source":["if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Finish reading brown sentences\n","finished all sentences and build index\n","word total: 2001\n","Start index=0 and End index = 1\n","Learing Rate is 0.1:\n","In iteration NO.0\n","epoch: 0, sentence: 0/2000, loss: 7.599008997760336\n","epoch: 0, sentence: 500/2000, loss: 6.171957576435965\n","epoch: 0, sentence: 1000/2000, loss: 4.892173366817531\n","epoch: 0, sentence: 1500/2000, loss: 7.794433794352203\n","Elapsed time training: 0:01:50.730097\n","In iteration NO.1\n","epoch: 1, sentence: 0/2000, loss: 6.025639437386274\n","epoch: 1, sentence: 500/2000, loss: 5.628708206556994\n","epoch: 1, sentence: 1000/2000, loss: 6.155335745686917\n","epoch: 1, sentence: 1500/2000, loss: 4.025352857706961\n","Elapsed time training: 0:03:39.501018\n","In iteration NO.2\n","epoch: 2, sentence: 0/2000, loss: 5.357859139694355\n","epoch: 2, sentence: 500/2000, loss: 4.952859658221779\n","epoch: 2, sentence: 1000/2000, loss: 4.253707545673519\n","epoch: 2, sentence: 1500/2000, loss: 5.265011888659899\n","Elapsed time training: 0:05:27.210212\n","In iteration NO.3\n","epoch: 3, sentence: 0/2000, loss: 4.797671340063463\n","epoch: 3, sentence: 500/2000, loss: 6.184429771245989\n","epoch: 3, sentence: 1000/2000, loss: 5.455672838197983\n","epoch: 3, sentence: 1500/2000, loss: 4.014481608942619\n","Elapsed time training: 0:07:14.218596\n","In iteration NO.4\n","epoch: 4, sentence: 0/2000, loss: 4.559625345884817\n","epoch: 4, sentence: 500/2000, loss: 5.587894704731957\n","epoch: 4, sentence: 1000/2000, loss: 7.0704062043402285\n","epoch: 4, sentence: 1500/2000, loss: 5.453914656179937\n","Elapsed time training: 0:09:00.289303\n","In iteration NO.5\n","epoch: 5, sentence: 0/2000, loss: 4.556433171623489\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oU-nIlXRC8rc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}