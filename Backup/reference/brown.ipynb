{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"brown.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QY6QK6ogsoli","colab_type":"text"},"source":["## Brown Corpus\n","\n","The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University.\n","This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.\n","\n","http://icame.uib.no/brown/bcm-los.html\n","\n","The corpus could be accessed as a list of words, or a list of sentences (each sentence is a list of words).\n","\n"]},{"cell_type":"code","metadata":{"id":"RgKU_LMpsolj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"46f10b36-5b77-4884-8a05-409e9c1d7c9c","executionInfo":{"status":"ok","timestamp":1569370353949,"user_tz":240,"elapsed":15957,"user":{"displayName":"Yanjun Li","photoUrl":"","userId":"03803147230974575947"}}},"source":["#from nltk.corpus import brown\n","import nltk\n","nltk.download()\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> brown\n","    Downloading package brown to /root/nltk_data...\n","      Package brown is already up-to-date!\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"TfuY7XUKsolm","colab_type":"code","colab":{}},"source":["from nltk.corpus import brown"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"phR1s8wosolo","colab_type":"code","colab":{}},"source":["from nltk import bigrams, trigrams, ngrams, word_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1PKmlezsolq","colab_type":"code","colab":{}},"source":["import operator\n","import os\n","import sys\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdBNd8rxsols","colab_type":"code","colab":{}},"source":["def get_brown_sentences():\n","    # returns 57340 of the Brown corpus\n","    # each sentence is represented as a list of individual string tokens\n","    return brown.sents()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPyvrAtBsolu","colab_type":"code","colab":{}},"source":["#convert sentences of words to list of indexes of words,\n","#the size of vocabulary could be set to a given number.\n","#the number of sentences could be set to a given number\n","\n","def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, num=0):\n","    if n_vocab<=2:\n","        print('vocabulary size should be larger than 2')\n","        system.exit(0)\n","        \n","    sentences = get_brown_sentences()\n","    print('Finish reading brown sentences')\n","    \n","    indexed_sentences = []#save processed sentences\n","\n","    #word to index mapping\n","    i = 2 #first two are fixed.\n","    word2idx = {'START': 0, 'END': 1}\n","    idx2word = ['START', 'END']\n","\n","    #dictionary for word's index and its counts\n","    word_idx_count = {\n","        0: float('inf'), \n","        #set their counts to big to keep them on top of the sorting results later\n","        1: float('inf'),\n","    }\n","\n","    #process each sentence at a time\n","    for sentence in sentences:\n","        indexed_sentence = []\n","        for token in sentence:\n","            token = token.lower()\n","            if token not in word2idx:#new token\n","                idx2word.append(token)\n","                word2idx[token] = i\n","                i += 1\n","                \n","\n","            # keep track of counts for later sorting\n","            idx = word2idx[token]\n","            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n","\n","            indexed_sentence.append(idx)\n","        indexed_sentences.append(indexed_sentence)\n","\n","    print('finished all sentences and build index')\n","\n","    # restrict vocab size\n","\n","    # set all the words I want to keep to infinity\n","    # so that they are included when I pick the most\n","    # common words\n","    \n","\n","    sorted_word_idx_count = sorted(word_idx_count.items(), \n","                        key=operator.itemgetter(1), reverse=True)\n","    word2idx_small = {}\n","    new_idx = 0\n","    idx_new_idx_map = {}\n","    for idx, count in sorted_word_idx_count[:n_vocab]:\n","        word = idx2word[idx]\n","        #print(word, count, end=\" |\")\n","        word2idx_small[word] = new_idx\n","        idx_new_idx_map[idx] = new_idx\n","        new_idx += 1\n","    #print(word2idx_small)\n","    # let 'unknown' be the last token\n","    word2idx_small['UNKNOWN'] = new_idx \n","    unknown = new_idx\n","\n","    assert('START' in word2idx_small)\n","    assert('END' in word2idx_small)\n","    \n","    # map old idx to new idx\n","    sentences_small = []\n","    number_s=0\n","    \n","    flag = True\n","    for sentence in indexed_sentences:\n","        \n","        if num == 0:\n","            flag = True\n","        elif num>0  and number_s < num:\n","            flag = True\n","        elif num >0 and number_s >= num:\n","            flag = False\n","        \n","        if flag == True and len(sentence) > 1:\n","            new_sentence = [idx_new_idx_map[idx] if idx \n","                            in idx_new_idx_map else unknown for idx in sentence]\n","            sentences_small.append(new_sentence)\n","            number_s +=1\n","\n","    return sentences_small, word2idx_small\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6nLRIg7solv","colab_type":"code","colab":{}},"source":["def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n","    #structure of bigram probability matrix will be\n","    #(last word-current word)--> probability\n","    #we will use add-one smoothing\n","    #ignore the end word\n","    bigram_probs=np.ones((V,V))*smoothing\n","    #size of V by V matrix, add one from the beginning\n","    #print(bigram_probs)\n","    for sentence in sentences:\n","        #print(len(sentence))\n","        for i in range(len(sentence)):\n","            if i==0:\n","                #begining word\n","                bigram_probs[start_idx,sentence[i]]+=1\n","            else:\n","                #middle word\n","                bigram_probs[sentence[i-1], sentence[i]]+=1\n","                \n","            #if we are at the final word\n","            #we update the bigram for last->current\n","            #and current-> end token\n","            if i == len(sentence)-1:\n","                #final word\n","                bigram_probs[sentence[i], end_idx]+=1\n","    #print(bigram_probs)\n","    #normalize the counts along the rows to get probabilities\n","    bigram_probs/=bigram_probs.sum(axis=1, keepdims=True)#sum per row\n","    #print(bigram_probs)\n","    return bigram_probs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LBhT4ccsolx","colab_type":"code","colab":{}},"source":["def get_bigram_probs_nltk(sentences, V, start_idx, end_idx, smoothing=1):\n","    bigram_probs = np.ones((V,V))*smoothing\n","    for sentence in sentences:\n","        bigram = list(ngrams(sentence,2))\n","        #print(bigram)\n","            \n","        bigram_probs[start_idx, bigram[0][0]]+=1\n","        bigram_probs[bigram[len(bigram)-1][1], end_idx]+=1\n","        for bg in bigram:\n","            bigram_probs[bg[0],bg[1]]+=1\n","    \n","    bigram_probs/=bigram_probs.sum(axis=1,keepdims=True)\n","    return bigram_probs\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1J93MDNvsolz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0Wdbx4Ysol1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}