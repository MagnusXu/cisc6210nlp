{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISC 6210 NLP HW1 by Zhiyu Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "The whole homework, in my design, has been splited into 3 parts:\n",
    "- Remotely download the poem link and save them locally.\n",
    "- Read the url file and get the poem text, then according to the task, finish the CleanOutputLoveOutput table.\n",
    "- Based on the clean table, finish the rest 4 ProcessedLoveOutput table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Get the poem link.\n",
    "The source page has all those poem files. I don't know if the storm server has any better ways for users to download the files from the server. My solution is to use the python web crawler package requests and bs4, to get the postfix of those poems' url, which happens to be their names.\n",
    "\n",
    "After I get those name, I save them into a local text file. In the following step, I read lines from that file and combine them with the prefix: 'https://storm.cis.fordham.edu/~yli/data/LoveOutput/'. After that, I can get all the contents I want to continue the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Based on the url and BeautifulSoup, get the soup object\n",
    "def get_soup(url):\n",
    "    html = requests.get(url)\n",
    "    txt = html.text\n",
    "    soup = BeautifulSoup(txt, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# Use the regular expression, find the poem file names we need from the html.\n",
    "def get_link(url):\n",
    "    soup = get_soup(url)\n",
    "    text = soup.findAll('table')[0].findAll('td')\n",
    "    rows = []\n",
    "    for x in text:\n",
    "        rows.append(str(x))\n",
    "    valid = []\n",
    "    for row in rows:\n",
    "        pattern = '<a href=.+</a>'\n",
    "        result = re.search(pattern, row)\n",
    "        if result:\n",
    "            link = result.group(0)\n",
    "            link = link.strip('\"')\n",
    "            valid.append(link)\n",
    "    return valid\n",
    "\n",
    "# The prefix of all poem urls, which follow by the poem name.\n",
    "url = 'https://storm.cis.fordham.edu/~yli/data/LoveOutput/'\n",
    "\n",
    "valid = get_link(url)\n",
    "with open('/Users/lordxuzhiyu/Desktop/valid_ip.txt', 'a') as f:\n",
    "    for item in valid:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above would create a local text file with all the poem names. I put that file in my submission folder.\n",
    "You need to change the local file path to test on your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. CleanOutputLoveOutput table.\n",
    "In this part, we need to combine the url with the local file first.\n",
    "\n",
    "We would use the iteration, for each line of the url file, we test the usability of the url, if it works, we follow the instruction to get all the information we need, and then save them into a row of record in a pandas DataFrame; if it doesn't work, we treat it as an exception and then jump to the next line of the file.\n",
    "\n",
    "As for the pandas operation, I would put proper annotations in the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Author, Title, Tags, Body, Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Combine the prefix url0 with the local poem name to get the full url of each poem.\n",
    "def combine_url(url1, url2):\n",
    "    return '%s%s' % (url1, url2)\n",
    "\n",
    "def get_text(url, head):\n",
    "    html = requests.get(url, headers = head)\n",
    "    txt = html.text\n",
    "    return txt\n",
    "\n",
    "head = {\n",
    "    'User-Agent':'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',\n",
    "}\n",
    "\n",
    "# url0 is the prefix.\n",
    "url0 = 'https://storm.cis.fordham.edu/~yli/data/LoveOutput/'\n",
    "\n",
    "table0 = pd.DataFrame(columns = ['Author', 'Title', 'Tags', 'Body', 'Link'])\n",
    "print(table0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fails:  422\n",
      "sum of poems 5439\n",
      "sum of different authors 906\n",
      "William Shakespeare                        123\n",
      "John Donne                                  87\n",
      "Robert Browning                             57\n",
      "Edmund Spenser                              57\n",
      "Sir  Thomas Wyatt                           54\n",
      "Anonymous                                   42\n",
      "Thomas Campion                              42\n",
      "Algernon Charles Swinburne                  42\n",
      "John Keats                                  39\n",
      "George Meredith                             39\n",
      "William Butler Yeats                        36\n",
      "Aphra Behn                                  33\n",
      "Alfred, Lord Tennyson                       33\n",
      "Andrew Marvell                              33\n",
      "Amy Lowell                                  33\n",
      "Robert Herrick                              30\n",
      "Sara Teasdale                               30\n",
      "Brenda Shaughnessy                          30\n",
      "Louise Gl�ck                                30\n",
      "Robert Burns                                30\n",
      "Dante Gabriel Rossetti                      27\n",
      "Robert Creeley                              27\n",
      "Thomas Hardy                                27\n",
      "Christina Rossetti                          27\n",
      "Yusef Komunyakaa                            24\n",
      "William Blake                               24\n",
      "Walt Whitman                                24\n",
      "Thomas Carew                                24\n",
      "Samuel Taylor Coleridge                     24\n",
      "Paisley Rekdal                              21\n",
      "                                          ... \n",
      "Daniel Borzutzky                             3\n",
      "Fady Joudah                                  3\n",
      "Alison Hawthorne Deming                      3\n",
      "Jessica Greenbaum                            3\n",
      "Trish Dugger                                 3\n",
      "Piotr Sommer                                 3\n",
      "Alan Dugan                                   3\n",
      "Wesley McNair                                3\n",
      "John Beer                                    3\n",
      "Cate Marvin                                  3\n",
      "Eugene Dubnov                                3\n",
      "Isabella Whitney                             3\n",
      "Jamila Woods                                 3\n",
      "Lia Purpura                                  3\n",
      "Nikky Finney                                 3\n",
      "Charles d&#039;Orleans                       3\n",
      "David Hernandez                              3\n",
      "Ben Estes                                    3\n",
      "Mary Jo Salter                               3\n",
      "Lady Mary Wroth                              3\n",
      "Moira Egan                                   3\n",
      "Mary Moore Easter                            3\n",
      "Weldon Kees                                  3\n",
      "Duchess of Newcastle Margaret Cavendish      3\n",
      "Duane Niatum                                 3\n",
      "Nate Klug                                    3\n",
      "J. M. Synge                                  3\n",
      "elena minor                                  3\n",
      "Cyrus Cassells                               3\n",
      "Gerald Stern                                 3\n",
      "Name: Author, Length: 906, dtype: int64\n",
      "                     Author Title                           Tags  \\\n",
      "count                  5439  5439                           5439   \n",
      "unique                  906  1385                           1526   \n",
      "top     William Shakespeare  Love  Love, Realistic & Complicated   \n",
      "freq                    123   135                             84   \n",
      "\n",
      "                                                     Body  \\\n",
      "count                                                5439   \n",
      "unique                                               1813   \n",
      "top     All my past life is mine no more,[L]      The ...   \n",
      "freq                                                    3   \n",
      "\n",
      "                                                     Link  \n",
      "count                                                5439  \n",
      "unique                                               1813  \n",
      "top     https://www.poetryfoundation.org/poems/50320/t...  \n",
      "freq                                                    3  \n"
     ]
    }
   ],
   "source": [
    "fails = 0\n",
    "\n",
    "# Use the iteration to access the url file.\n",
    "for line in open('/Users/lordxuzhiyu/Desktop/valid_ip.txt'):\n",
    "    url = combine_url(url0, line)\n",
    "    url = url.replace(' ', '').replace('\\n', '').replace('\\t', '').replace('\\r', '').strip()\n",
    "    try:\n",
    "        # First step to test the usability of the url.\n",
    "        p = requests.get(url, headers = head, timeout = 5)\n",
    "        \n",
    "        html = requests.get(url,headers=head)\n",
    "        txt = html.text\n",
    "        text = txt.splitlines()\n",
    "        \n",
    "        # Title\n",
    "        title = re.findall(r'(\\w+) By', text[0]) \n",
    "        # Author\n",
    "        author = re.findall(r'By (.+)', text[0])\n",
    "        # Tags\n",
    "        tag = text[2].split(' ;')\n",
    "        tag = ', '.join(tag)\n",
    "        # Body\n",
    "        body = text[4].replace('<br><br>', '[P]')\n",
    "        body = body.replace('<br>', '[L]')\n",
    "        # Link\n",
    "        link = re.findall(r'http.+', text[6])  \n",
    "        \n",
    "        record = {'Author': author[0], 'Title': title, 'Tags': tag, 'Body': body, 'Link': link[0]}\n",
    "        temp = pd.DataFrame(record, index = [0])\n",
    "        table0 = table0.append(temp, ignore_index = True)\n",
    "    \n",
    "    except Exception as error:\n",
    "        fails += 1\n",
    "        #print(error)\n",
    "        continue\n",
    "\n",
    "# The total failure urls.\n",
    "print('Fails: ', fails)\n",
    "\n",
    "# sum of poems\n",
    "print('sum of poems', len(table0.index))\n",
    "\n",
    "# sum of different authors\n",
    "sum_author = len(table0['Author'].unique().tolist())\n",
    "print('sum of different authors', sum_author)\n",
    " \n",
    "# author sorted\n",
    "print(table0['Author'].value_counts())\n",
    "\n",
    "# fully description\n",
    "print(table0.describe())\n",
    "\n",
    "# Save the dataframe to local csv file, then transform it to the xlsx file.\n",
    "table0.to_csv('/Users/localpath/CleanOutputLoveOutput.csv', \n",
    "              index = None, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. ProcessedLoveOutput table.\n",
    "Use the nltk package to finish the requirements.\n",
    "This part is actually easy and clear.\n",
    "\n",
    "For the punctuation rule part, I use the regular expression. e.g. \"I'm\", the colon would be seen as a punction mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PoemID                      Author LengthOne LengthTwo NumLine NumPara  \\\n",
      "0         0           Suzanne Gardinier        76        74      14       7   \n",
      "1         1          Jeff Daniel Marion        72        67      16       1   \n",
      "2         2               Roddy Lumsden       205       198      32      16   \n",
      "3         3               Dennis Cooper       130       120      39       1   \n",
      "4         4               Kathryn Maris       188       175      24       8   \n",
      "5         5                Thomas Moore       203       191      40       8   \n",
      "6         6          Christina Rossetti        74        69      17       2   \n",
      "7         7             Jane Hirshfield       108       103      23       1   \n",
      "8         8               Lewis Carroll        88        81      21       7   \n",
      "9         9          Robert VanderMolen       139       132      38      22   \n",
      "10       10                 Cate Marvin       202       194      35       5   \n",
      "11       11                 Leigh Stein       171       164      38      19   \n",
      "12       12                Thomas Hardy        97        89      17       2   \n",
      "13       13            Clifton Gachagua        98        93      12       1   \n",
      "14       14  Algernon Charles Swinburne        96        90      14       1   \n",
      "15       15                  Ben Jonson       141       133      31       3   \n",
      "16       16            Isabella Whitney       149       144      36       1   \n",
      "17       17              George Chapman        88        83      15       1   \n",
      "18       18              Kevin McFadden       113       102      15       1   \n",
      "19       19           Sir  Thomas Wyatt        62        57       9       2   \n",
      "20       20              Andrew Marvell       226       216      68      16   \n",
      "21       21          Christina Rossetti        63        58      13       3   \n",
      "22       22              Joanna Baillie       439       431      87       3   \n",
      "23       23        William Butler Yeats        36        31       6       1   \n",
      "24       24               Matthew Prior        97        88      15       3   \n",
      "25       25             Robert Browning       147       138      23       1   \n",
      "26       26           Sir Walter Ralegh       166       159      31       5   \n",
      "27       27                Charles Lamb       505       490     146       1   \n",
      "28       28                Norman Dubie       170       160      27       5   \n",
      "29       29                  Amy Lowell        96        90      15       1   \n",
      "...     ...                         ...       ...       ...     ...     ...   \n",
      "1783   1783        Francisco X. Alarc�n        73        70      15       5   \n",
      "1784   1784             Lady Mary Wroth        94        88      14       4   \n",
      "1785   1785               William Blake        19        15       4       1   \n",
      "1786   1786                Emily Bront�        44        39       7       1   \n",
      "1787   1787                 Nate Pritts        68        59      13       1   \n",
      "1788   1788                Rafael Campo       178       168      38       1   \n",
      "1789   1789                   Anonymous       195       181      60       5   \n",
      "1790   1790               Louis Simpson       270       262      78      25   \n",
      "1791   1791       Alfred, Lord Tennyson       110       104      24       8   \n",
      "1792   1792         William Shakespeare       405       396      85      13   \n",
      "1793   1793                     Solomon       180       173      18       1   \n",
      "1794   1794                  Aphra Behn       153       145      28       5   \n",
      "1795   1795                    Tibullus       176       169      32       3   \n",
      "1796   1796          Valerie Mejer Caso       176       162      29       7   \n",
      "1797   1797         Quraysh Ali Lansana        72        70      14       4   \n",
      "1798   1798                 Dick Lourie        97        95      13       4   \n",
      "1799   1799               Tyehimba Jess       111       107      25       4   \n",
      "1800   1800             Andrew McMillan        97        86      12       1   \n",
      "1801   1801       Reina Mar�a Rodr�guez       463       454       1       0   \n",
      "1802   1802               Nate Marshall       175       165      28       1   \n",
      "1803   1803                 Patti Smith       151       145      30       2   \n",
      "1804   1804                 Tom Pickard        13        11       5       2   \n",
      "1805   1805                 Danez Smith       186       174      50      26   \n",
      "1806   1806              Kirby Knowlton       172       161      10       9   \n",
      "1807   1807        Olena Kalytiak Davis       273       259      54      18   \n",
      "1808   1808              Rodney Koeneke        56        51      13       1   \n",
      "1809   1809                   Tory Dent       140       133      56       2   \n",
      "1810   1810              Kirby Knowlton       126       115       5       4   \n",
      "1811   1811  Julian Talamantez Brolaski        90        87      17       1   \n",
      "1812   1812               George Quasha       119       112      15       8   \n",
      "\n",
      "     NumSent NumComma  \n",
      "0          1        0  \n",
      "1          2       10  \n",
      "2         17       19  \n",
      "3         25        5  \n",
      "4          6       26  \n",
      "5          8       30  \n",
      "6          3        3  \n",
      "7          1        3  \n",
      "8          5       11  \n",
      "9          2        2  \n",
      "10        17       27  \n",
      "11        14       16  \n",
      "12         4       11  \n",
      "13        10       11  \n",
      "14         5        9  \n",
      "15        17       16  \n",
      "16         9       24  \n",
      "17         3        8  \n",
      "18        11       12  \n",
      "19         2        2  \n",
      "20        25       34  \n",
      "21         5        7  \n",
      "22        25       71  \n",
      "23         3        2  \n",
      "24         3       16  \n",
      "25         5       22  \n",
      "26         7       33  \n",
      "27        20      138  \n",
      "28        15        4  \n",
      "29         6        7  \n",
      "...      ...      ...  \n",
      "1783       1       10  \n",
      "1784       2        9  \n",
      "1785       5        0  \n",
      "1786       2        3  \n",
      "1787       6        7  \n",
      "1788      16       21  \n",
      "1789      13        4  \n",
      "1790      20       30  \n",
      "1791       9       35  \n",
      "1792      29       71  \n",
      "1793      19       43  \n",
      "1794      11       14  \n",
      "1795       8       28  \n",
      "1796      15       18  \n",
      "1797       1        0  \n",
      "1798       1        0  \n",
      "1799       5       11  \n",
      "1800       1       15  \n",
      "1801      49       74  \n",
      "1802      25        6  \n",
      "1803      31        0  \n",
      "1804       1        0  \n",
      "1805      38       18  \n",
      "1806      14        6  \n",
      "1807      40       88  \n",
      "1808       7        7  \n",
      "1809       2       26  \n",
      "1810      10        6  \n",
      "1811       1        4  \n",
      "1812      15        3  \n",
      "\n",
      "[1813 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# read the local file\n",
    "clean = pd.read_csv('/Users/lordxuzhiyu/Desktop/CleanOutputLoveOutput.csv')\n",
    "\n",
    "# create a dataframe to store the record\n",
    "df = pd.DataFrame(columns = ['PoemID', 'Author', 'LengthOne', 'LengthTwo',\n",
    "                             'NumLine', 'NumPara', 'NumSent', 'NumComma'])\n",
    "\n",
    "for index, row in clean.iterrows():\n",
    "    #print(index)\n",
    "    poemID = index\n",
    "    author = row['Author']\n",
    "    \n",
    "    tokens = tokens = word_tokenize(row['Body'])\n",
    "    len1 = len(set(tokens))\n",
    "    \n",
    "    # regular expression pattern to match the non-punctuation tokens.\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')\n",
    "    tokens2 = [w for w in tokens if nonPunct.match(w)]\n",
    "    len2 = len(set(tokens2))\n",
    "    \n",
    "    p = row['Body'].count('[P]')\n",
    "    l = row['Body'].count('[L]')\n",
    "    numL = l + p\n",
    "    numP = p\n",
    "    \n",
    "    sen = sent_tokenize(row['Body'])\n",
    "    comma = row['Body'].count(',')\n",
    "    \n",
    "    raw = {'PoemID': poemID, 'Author': author, 'LengthOne': len1,\n",
    "              'LengthTwo': len2, 'NumLine': numL, 'NumPara': numP, \n",
    "              'NumSent':len(sen), 'NumComma': comma}\n",
    "    record = pd.DataFrame(raw, index = [0])\n",
    "    \n",
    "    df = df.append(record, ignore_index = True)\n",
    "\n",
    "print(df)\n",
    "df.to_csv('/Users/lordxuzhiyu/Desktop/ProcessedLoveOutput0.csv', \n",
    "              index = None, header = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The rest three tables.\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "clean = pd.read_csv('/Users/localpath/CleanOutputLoveOutput.csv')\n",
    "\n",
    "df1 = pd.DataFrame(columns = ['PoemID', 'Author', 'Body', 'Length', 'UniCount'])\n",
    "df2 = pd.DataFrame(columns = ['PoemID', 'Author', 'Body', 'Length', 'UniCount'])\n",
    "df3 = pd.DataFrame(columns = ['PoemID', 'Author', 'Body', 'Length', 'UniCount'])\n",
    "\n",
    "for index, row in clean.iterrows():\n",
    "    print(index)\n",
    "    poemID = index\n",
    "    author = row['Author']\n",
    "    \n",
    "    tokens = word_tokenize(row['Body'])\n",
    "    body1 = ' '.join(tokens)\n",
    "    len1 = len(tokens)\n",
    "    uni1 = len(set(tokens))\n",
    "    \n",
    "    clean_tokens = list()\n",
    "    sr = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if not token in sr:\n",
    "            clean_tokens.append(token)\n",
    "    body2 = ' '.join(clean_tokens)\n",
    "    len2 = len(clean_tokens)\n",
    "    uni2 = len(set(clean_tokens))\n",
    "    \n",
    "    tab3 = []\n",
    "    ps = PorterStemmer() \n",
    "    stems = clean_tokens\n",
    "    for w in stems:\n",
    "        w = ps.stem(w)\n",
    "        tab3.append(w)\n",
    "    body3 = ' '.join(tab3)\n",
    "    len3 = len(tab3)\n",
    "    uni3 = len(set(tab3))   \n",
    "    \n",
    "    raw1 = {'PoemID': poemID, 'Author': author, 'Body': body1,\n",
    "            'Length': len1, 'UniCount': uni1}\n",
    "    record1 = pd.DataFrame(raw1, index = [0])\n",
    "    df1 = df1.append(record1, ignore_index = True)\n",
    "    \n",
    "    raw2 = {'PoemID': poemID, 'Author': author, 'Body': body2,\n",
    "            'Length': len2, 'UniCount': uni2}\n",
    "    record2 = pd.DataFrame(raw2, index = [0])\n",
    "    df2 = df2.append(record2, ignore_index = True)\n",
    "    \n",
    "    raw3 = {'PoemID': poemID, 'Author': author, 'Body': body3,\n",
    "            'Length': len3, 'UniCount': uni3}\n",
    "    record3 = pd.DataFrame(raw3, index = [0])\n",
    "    df3 = df1.append(record3, ignore_index = True)\n",
    "\n",
    "df1.to_csv('/Users/lordxuzhiyu/Desktop/ProcessedLoveOutput1.csv', \n",
    "              index = None, header = True)  \n",
    "df2.to_csv('/Users/lordxuzhiyu/Desktop/ProcessedLoveOutput2.csv', \n",
    "              index = None, header = True)  \n",
    "df3.to_csv('/Users/lordxuzhiyu/Desktop/ProcessedLoveOutput3.csv', \n",
    "              index = None, header = True)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
