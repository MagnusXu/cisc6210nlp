#%%[markdown]
## First Page

#%%
from sklearn.model_selection import train_test_split
import requests
from bs4 import BeautifulSoup as bs
import random, re

head = {
    'User-Agent': 'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)'
}

def html_text(url):
    html = requests.get(url, headers = head)
    source = html.text
    soup = bs(source, 'lxml')
    raw_text = soup.find_all('review_text')
    raw_list = [l.text for l in raw_text]
    result = ' '.join(raw_list)
    return result.lower()

positive_url = 'https://storm.cis.fordham.edu/~yli/data/electronics/positive.review'
negative_url = 'https://storm.cis.fordham.edu/~yli/data/electronics/negative.review'
good_text = html_text(positive_url)
bad_text = html_text(negative_url)

#%%
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
good_tokens = tokenizer.tokenize(good_text)
bad_tokens = tokenizer.tokenize(bad_text)

from nltk.stem import PorterStemmer
ps = PorterStemmer()
good_tokens = [ps.stem(g) for g in good_tokens]
bad_tokens = [ps.stem(b) for b in bad_tokens]

print(good_tokens)

#%%
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
good_filtered = [g for g in good_tokens if not g in stop_words]
bad_filtered = [b for b in bad_tokens if not b in stop_words]

print(good_filtered)

#%%
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud

def show_wordcloud(text):
    text_list = Counter(text)
    d = {}
    for k, v in text_list:
        d[k] = int(v)
    wordcloud = WordCloud().generate_from_frequencies(d)
    fig = plt.figure(1, figsize = (20, 20))
    plt.axis('off')
    plt.imshow(wordcloud)
    plt.show()
    pass

def shared_words(list1, list2):
    l1 = {}
    l2 = {}
    for k, v in list1:
        l1[k] = int(v)
    for k, v in list2:
        l2[k] = int(v)
    return list(set(l1) & set(l2))

ns = [20, 50, 100]
for n in ns:
    print('Most Common', n)
    gm = Counter(good_tokens).most_common(n)
    bm = Counter(bad_tokens).most_common(n)
    #result = gm + bm
    show_wordcloud(gm + bm)
    shared = shared_words(gm, bm)
    print(shared)
    print('Total common words: ', len(shared))

#%%
def countList(lst, sublist):
    d = {}
    for x in sublist:
        count = sum(x in item for item in lst)
        d[x] = int(count)
    return d

gm = Counter(good_tokens).most_common(200)
bm = Counter(bad_tokens).most_common(200)
gfm = Counter(good_filtered).most_common(200)
bfm = Counter(bad_filtered).most_common(200)

filter_shared = shared_words(gfm, bfm)
non_f_shared = shared_words(gm, bm)

print(filter_shared)
print(non_f_shared)
#%%
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

data = []
labels = []

for i in good_filtered:
    data.append(i)
    labels.append('pos')

for i in bad_filtered:
    data.append(i)
    labels.append('neg')

vectorizer = CountVectorizer(
    analyzer = 'word',
    lowercase = False,
)

features = vectorizer.fit_transform(data)
features_nd = features.toarray()

X_train, X_test, y_train, y_test = train_test_split(
    features_nd,
    labels,
    train_size=0.80,
    random_state=1234)


#%%
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics

gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
print('Precision:', metrics.precision_score(y_test, y_pred, average='macro'))
print("Recall:", metrics.recall_score(y_test, y_pred, average='macro'))
print('F-score:', metrics.f1_score(y_test, y_pred, average='macro'))

#%%
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best cross-validation score: {:.2f}".format(grid.best_score_))
print("Best parameters: ", grid.best_params_)
print("Best estimator: ", grid.best_estimator_)
lr = grid.best_estimator_
lr.fit(X_train, y_train)
lr.predict(X_test)
print("Score: {:.2f}".format(lr.score(X_test, y_test)))

#%%
import mglearn
mglearn.tools.visualize_coefficients(
    grid.best_estimator_.coef_, feature_names, n_top_features=25)
plt.show()
